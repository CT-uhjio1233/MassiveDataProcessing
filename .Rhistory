#建立混淆矩陣(confusion,matrix)觀察模型表現
cm <- table(testdata$status,result,dnn=c("實際","預測"))
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/DecisionTreeForWin10.R')
setwd("D:/r place")
#測試模型,可隨機產生(訓練資料,測試資料)
traindata=read.csv("wdbctrain.csv")
testdata=read.csv("wdbctest.csv")
require(rpart)
#建立決策樹模型;(因變數~自變數)
DataTree <- rpart(status~., data = traindata, method = "class")
#畫決策樹
require(rpart.plot)
prp(DataTree,          #模型
faclen=0,          #呈現的變數不要縮寫
fallen.leaves=TRUE,#讓樹枝以垂直的方式呈現
shadow.col="gray", #最下方的節點塗上陰影
extra=2)
#預測
result <- predict(DataTree,newdata=testdata[-20],type="class")
#建立混淆矩陣(confusion,matrix)觀察模型表現
cm <- table(testdata$status,result,dnn=c("實際"))
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/DecisionTreeForWin10.R')
#測試模型,可隨機產生(訓練資料,測試資料)
traindata=read.csv("wdbctrain.csv")
testdata=read.csv("wdbctest.csv")
print(traindata)
#建立決策樹模型;(因變數~自變數)
DataTree <- rpart(status~, data = traindata, method = "class")
testdata=read.csv("wdbctest.csv")
print(traindata)
require(rpart)
#建立決策樹模型;(因變數~自變數)
DataTree <- rpart(status~., data = traindata, method = "class")
#畫決策樹
require(rpart.plot)
prp(DataTree,          #模型
faclen=0,          #呈現的變數不要縮寫
fallen.leaves=TRUE,#讓樹枝以垂直的方式呈現
shadow.col="gray", #最下方的節點塗上陰影
extra=2)
fancyRpartPlot(DataTree)
#預測
result <- predict(DataTree,newdata=testdata,type="class")
#預測
print(DataTree)
result <- predict(DataTree,newdata=testdata,type="class")
#測試模型,可隨機產生(訓練資料,測試資料)
traindata=read.csv("wdbctrain.csv")
testdata=read.csv("wdbctest.csv")
print(traindata)
require(rpart)
#建立決策樹模型;(因變數~自變數)
DataTree <- rpart(status~., data = traindata, method = "class")
#建立決策樹模型;(因變數~自變數)
DataTree <- rpart(status~., data = traindata, method = "class")
print(status~.)
#畫決策樹
require(rpart.plot)
print(traindata)
require(rpart)
#建立決策樹模型;(因變數~自變數)
DataTree <- rpart(status~., data = traindata, method = "class")
print(rpart(status~.))
source('D:/r place/DecisionTreeForWin10.R')
result <- predict(DataTree,newdata=testdata,type="class")
#建立混淆矩陣(confusion,matrix)觀察模型表現
cm <- table(testdata$status,result,dnn=c("實際","預測"))
print(cm)
setwd("D:/r place")
#測試模型,可隨機產生(訓練資料,測試資料)
traindata=read.csv("wdbctrain.csv")
testdata=read.csv("wdbctest.csv")
print(traindata)
require(rpart)
#建立決策樹模型;(因變數~自變數)
DataTree <- rpart(status~., data = traindata, method = "class")
print(DataTree)
setwd("D:/r place")
#測試模型,可隨機產生(訓練資料,測試資料)
traindata=read.csv("wdbctrain.csv")
testdata=read.csv("wdbctest.csv")
require(rpart)
#建立決策樹模型;(因變數~自變數)
DataTree <- rpart(status~., data = traindata, method = "class")
print(DataTree)
#畫決策樹
require(rpart.plot)
prp(DataTree,          #模型
faclen=0,          #呈現的變數不要縮寫
fallen.leaves=TRUE,#讓樹枝以垂直的方式呈現
shadow.col="gray", #最下方的節點塗上陰影
extra=2)
fancyRpartPlot(DataTree)
result <- predict(DataTree,newdata=testdata,type="class")
result <- predict(DataTree,type="class")
#建立混淆矩陣(confusion,matrix)觀察模型表現
cm <- table(testdata$status,result,dnn=c("實際","預測"))
source('D:/r place/DecisionTreeForWin10.R')
result <- predict(DataTree,newdata=testdata,type="class")
#建立混淆矩陣(confusion,matrix)觀察模型表現
cm <- table(testdata$status,result,dnn=c("實際","預測"))
source('D:/r place/R_Code_ANNforWin10.R', encoding = 'UTF-8')
source('D:/r place/R_Code_ANNforWin10.R', encoding = 'UTF-8')
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/R_Code_ANNforWin10.R', encoding = 'UTF-8')
source('D:/r place/R_Code_ANNforWin10.R', encoding = 'UTF-8')
source('D:/r place/R_Code_ANNforWin10.R', encoding = 'UTF-8')
source('D:/r place/R_Code_ANNforWin10.R', encoding = 'UTF-8')
source('D:/r place/R_Code_ANNforWin10.R', encoding = 'UTF-8')
source('D:/r place/R_Code_ANNforWin10.R', encoding = 'UTF-8')
source('D:/r place/NeuralNetworkForFinalReport.R', encoding = 'UTF-8')
source('D:/r place/NeuralNetworkForFinalReport.R', encoding = 'UTF-8')
source('D:/r place/NeuralNetworkForFinalReport.R', encoding = 'UTF-8')
source('D:/r place/NeuralNetworkForFinalReport.R', encoding = 'UTF-8')
source('D:/r place/NeuralNetworkForFinalReport.R', encoding = 'UTF-8')
source('D:/r place/NeuralNetworkForFinalReport.R', encoding = 'UTF-8')
source('D:/r place/NeuralNetworkForFinalReport.R', encoding = 'UTF-8')
source('D:/r place/NeuralNetworkForFinalReport.R', encoding = 'UTF-8')
source('D:/r place/NeuralNetworkForFinalReport.R', encoding = 'UTF-8')
source('D:/r place/NeuralNetworkForFinalReport.R', encoding = 'UTF-8')
source('D:/r place/NeuralNetworkForFinalReport.R', encoding = 'UTF-8')
source('D:/r place/NeuralNetworkForFinalReport.R', encoding = 'UTF-8')
source('D:/r place/NeuralNetworkForFinalReport.R', encoding = 'UTF-8')
source('D:/r place/NeuralNetworkForFinalReport.R', encoding = 'UTF-8')
source('D:/r place/NeuralNetworkForFinalReport.R', encoding = 'UTF-8')
source('D:/r place/NeuralNetworkForFinalReport.R', encoding = 'UTF-8')
source('D:/r place/NeuralNetworkForFinalReport.R', encoding = 'UTF-8')
source('D:/r place/NeuralNetworkForFinalReport.R', encoding = 'UTF-8')
source('D:/r place/NeuralNetworkForFinalReport.R', encoding = 'UTF-8')
source('D:/r place/NeuralNetworkForFinalReport.R', encoding = 'UTF-8')
source('D:/r place/NeuralNetworkForFinalReport.R', encoding = 'UTF-8')
source('D:/r place/NeuralNetworkForFinalReport.R', encoding = 'UTF-8')
source('D:/r place/NeuralNetworkForFinalReport.R', encoding = 'UTF-8')
source('D:/r place/NeuralNetworkForFinalReport.R', encoding = 'UTF-8')
source('D:/r place/NeuralNetworkForFinalReport.R', encoding = 'UTF-8')
source('D:/r place/NeuralNetworkForFinalReport.R', encoding = 'UTF-8')
source('D:/r place/NeuralNetworkForFinalReport.R', encoding = 'UTF-8')
source('D:/r place/NeuralNetworkForFinalReport.R', encoding = 'UTF-8')
source('D:/r place/NeuralNetworkForFinalReport.R', encoding = 'UTF-8')
source('D:/r place/NeuralNetworkForFinalReport.R', encoding = 'UTF-8')
source('D:/r place/NeuralNetworkForFinalReport.R', encoding = 'UTF-8')
source('D:/r place/NeuralNetworkForFinalReport.R', encoding = 'UTF-8')
source('D:/r place/NeuralNetworkForFinalReport.R', encoding = 'UTF-8')
source('D:/r place/NeuralNetworkForFinalReport.R', encoding = 'UTF-8')
source('D:/r place/NeuralNetworkForFinalReport.R', encoding = 'UTF-8')
require(neuralnet)#for neuralnet(),nn model
require(nnet)#for class.ind()
require(caret)#for train(),tune paramenters
setwd("D:/r place")#設定工作目錄
traindata=read.csv("LR_train.csv")
testdata=read.csv("LR_test.csv")
testdata <- testdata[-c(1)]
traindata <- traindata[-c(1)]
#原始資料就會變成像這樣
head(traindata)
formula.bpn <- as.formula(status ~ x-box+y-box+width+high+onpix+x-bar+y-bar+x2bar+y2bar+xybar+x2ybr+xy2br+x-ege+xegvy+y-ege+yegvy)
bpn <- neuralnet(data=traindata,
formula=formula.bpn,
hidden=c(2),#一個隱藏層:2個node
learningrate=0.01,#leearing rate
threshold=0.01,#partial derivatives of the error function, a stopping criteria
stepmax=5e5#最大的iteration數=500000(5*10^5)
)
plot(bpn)
pred<- compute(bpn,testdata[1:22])
pred$net.result
#pred.result<-round(pre$net.result)
pred.result<-round(1/(1+exp(-pred$net.result)))
pred.result
cm <- table(testdata$status,pred.result)
print(cm)
if(length(cm)==4){
mycolNames<-colnames(cm)
mycolName[1]
precision <- (cm[[1]]/sum(cm[,1]))
paste("預測",mycolName[1],"的正確率,precision=",precision)
TPR <- (cm[[1]]/sum(cm[1,]))
print(paste("預測",mycolName[1],"的正確率,TPR=",TPR))
TNR <- (cm[[4]]/sum(cm[2,]))
print(paste("預測",mycolName[1],"的正確率,TNR=",TNR))
precision <- (cm[[4]]/sum(cm[,2]))
paste("預測",mycolName[1],"的正確率,precision=",precision)
TPR <- (cm[[4]]/sum(cm[2,]))
print(paste("預測",mycolName[1],"的正確率,TPR=",TPR))
TNR <- (cm[[1]]/sum(cm[1,]))
print(paste("預測",mycolName[1],"的正確率,TNR=",TNR))
accuracy <- sum(diag(cm))/sum(cm)
print(paste("整體準確率=",round(accuracy,2)))
}else{
print("confusuion matrix 的個數<4,需額外計算")
}
source('D:/r place/NeuralNetworkForFinalReport.R', encoding = 'UTF-8')
source('D:/r place/NeuralNetworkForFinalReport.R', encoding = 'UTF-8')
source('D:/r place/NeuralNetworkForFinalReport.R', encoding = 'UTF-8')
require(neuralnet)#for neuralnet(),nn model
require(nnet)#for class.ind()
require(caret)#for train(),tune paramenters
setwd("D:/r place")#設定工作目錄
traindata=read.csv("LR_train.csv")
testdata=read.csv("LR_test.csv")
testdata <- testdata[-c(1)]
traindata <- traindata[-c(1)]
#原始資料就會變成像這樣
head(traindata)
formula.bpn <- as.formula(paste("status ~",paste( xbox+ybox+width+high+onpix+x-bar+y-bar+x2bar+y2bar+xybar+x2ybr+xy2br+x-ege+xegvy+y-ege+yegvy)))
bpn <- neuralnet(data=traindata,
formula=formula.bpn,
hidden=c(2),#一個隱藏層:2個node
learningrate=0.01,#leearing rate
threshold=0.01,#partial derivatives of the error function, a stopping criteria
stepmax=5e5#最大的iteration數=500000(5*10^5)
)
plot(bpn)
pred<- compute(bpn,testdata[1:22])
pred$net.result
#pred.result<-round(pre$net.result)
pred.result<-round(1/(1+exp(-pred$net.result)))
pred.result
cm <- table(testdata$status,pred.result)
print(cm)
if(length(cm)==4){
mycolNames<-colnames(cm)
mycolName[1]
precision <- (cm[[1]]/sum(cm[,1]))
print(paste("預測",mycolName[1],"的正確率,precision=",precision))
TPR <- (cm[[1]]/sum(cm[1,]))
print(paste("預測",mycolName[1],"的正確率,TPR=",TPR))
TNR <- (cm[[4]]/sum(cm[2,]))
print(paste("預測",mycolName[1],"的正確率,TNR=",TNR))
precision <- (cm[[4]]/sum(cm[,2]))
paste("預測",mycolName[1],"的正確率,precision=",precision)
TPR <- (cm[[4]]/sum(cm[2,]))
print(paste("預測",mycolName[1],"的正確率,TPR=",TPR))
TNR <- (cm[[1]]/sum(cm[1,]))
print(paste("預測",mycolName[1],"的正確率,TNR=",TNR))
accuracy <- sum(diag(cm))/sum(cm)
print(paste("整體準確率=",round(accuracy,2)))
}else{
print("confusuion matrix 的個數<4,需額外計算")
}
source('D:/r place/NeuralNetworkForFinalReport.R', encoding = 'UTF-8')
source('D:/r place/NeuralNetworkForFinalReport.R', encoding = 'UTF-8')
source('D:/r place/NeuralNetworkForFinalReport.R', encoding = 'UTF-8')
source('D:/r place/NeuralNetworkForFinalReport.R', encoding = 'UTF-8')
source('D:/r place/NeuralNetworkForFinalReport.R', encoding = 'UTF-8')
source('D:/r place/R_Code_ANNforWin10.R')
source('D:/r place/NeuralNetworkForFinalReport.R', encoding = 'UTF-8')
source('D:/r place/NeuralNetworkForFinalReport.R', encoding = 'UTF-8')
source('D:/r place/R_Code_ANNforWin10.R')
require(neuralnet)#for neuralnet(),nn model
require(nnet)#for class.ind()
require(caret)#for train(),tune paramenters
setwd("D:/r place")#設定工作目錄
traindata=read.csv("LR_train.csv")
testdata=read.csv("LR_test.csv")
testdata <- testdata[-c(1)]
traindata <- traindata[-c(1)]
#原始資料就會變成像這樣
head(traindata)
formula.bpn <- as.formula(status ~ xbox+ybox+width+high+onpix+xbar+ybar+x2bar+y2bar+xybar+x2ybr+xy2br+x-ege+xegvy+y-ege+yegvy)
bpn <- neuralnet(data=traindata,
formula=formula.bpn,
hidden=c(2),#一個隱藏層:2個node
learningrate=0.01,#leearing rate
threshold=0.01,#partial derivatives of the error function, a stopping criteria
stepmax=5e5#最大的iteration數=500000(5*10^5)
)
plot(bpn)
require(neuralnet)#for neuralnet(),nn model
require(nnet)#for class.ind()
require(caret)#for train(),tune paramenters
setwd("D:/r place")#設定工作目錄
traindata=read.csv("LR_train.csv")
testdata=read.csv("LR_test.csv")
testdata <- testdata[-c(1)]
traindata <- traindata[-c(1)]
#原始資料就會變成像這樣
head(traindata)
formula.bpn <- as.formula(status ~ xbox+ybox+width+high+onpix+xbar+ybar+x2bar+y2bar+xybar+x2ybr+xy2br+x-ege+xegvy+y-ege+yegvy)
bpn <- neuralnet(data=traindata,
formula=formula.bpn,
hidden=c(2),#一個隱藏層:2個node
learningrate=0.01,#leearing rate
threshold=0.01,#partial derivatives of the error function, a stopping criteria
stepmax=5e5#最大的iteration數=500000(5*10^5)
)
plot(bpn)
pred<- compute(bpn,testdata[1:22])
pred$net.result
require(neuralnet)#for neuralnet(),nn model
require(nnet)#for class.ind()
require(caret)#for train(),tune paramenters
setwd("D:/r place")#設定工作目錄
traindata=read.csv("LR_train.csv")
testdata=read.csv("LR_test.csv")
testdata <- testdata[-c(1)]
traindata <- traindata[-c(1)]
#原始資料就會變成像這樣
head(traindata)
formula.bpn <- as.formula(status ~ xbox+ybox+width+high+onpix+xbar+ybar+x2bar+y2bar+xybar+x2ybr+xy2br+x-ege+xegvy+y-ege+yegvy)
bpn <- neuralnet(data=traindata,
formula=formula.bpn,
hidden=c(2),#一個隱藏層:2個node
learningrate=0.01,#leearing rate
threshold=0.01,#partial derivatives of the error function, a stopping criteria
stepmax=5e5#最大的iteration數=500000(5*10^5)
)
plot(bpn)
source('D:/r place/R_Code_ANNforWin10.R')
source('D:/r place/knnForW10.R')
source('D:/r place/knnForW10.R')
source('D:/r place/knnForW10.R')
source('D:/r place/knnForW10.R')
install.packages(c("ellipsis", "glue", "rlang", "tidyr", "vctrs"))
install.packages(c("ellipsis", "glue", "rlang", "tidyr", "vctrs"))
install.packages("dplyr")
if(!(packageName%in%rownames(installed.packages()))){
install.packages(packageName)
}
library(class)
library(dplyr)
setwd("D:/r place")
#測試模型,可隨機產生(訓練資料,測試資料)
traindata=read.csv("Parkinsons_TestANN.csv")
testdata=read.csv("Parkinsons_TestANN.csv")
#去除第一欄PK不適合分岔屬性
testdata <- testdata[-c(1)]
traindata <- traindata[-c(1)]
trainLabels <- traindata$status
knnTrain <- traindata[,-c(24)]
knnTest <- testdata[,-c(24)]
kv <- round(sqrt(length(knnTrain)))
prediction <- knn(train = knnTrain, test = knnTest, cl = trainLabels, k=kv)
cm <- table(testdata$status,prediction,dnn=c("實際","預測"))
print(cm)
if(length(cm)==4){
nycolName <- colnames(cm)
mycolName[1]
precision <- (cm[[1]]/sum(cm[,1]))
print(paste("預測",mycolName[1],"的正確率,precision=",round(precision,2)))
TPR <- (cm[[1]]/sum(cm[1,]))
print(paste("預測",mycolName[1],"的正確率,TPR=",round(TPR,2)))
TNR <- (cm[[4]]/sum(cm[2,]))
print(paste("預測",mycolName[1],"的正確率,TNR=",round(TNR,2)))
precision <- (cm[[4]]/sum(cm[,2]))
print(paste("預測",mycolName[2],"的正確率,precision=",round(precision,2)))
TPR <- (cm[[4]]/sum(cm[2,]))
print(paste("預測",mycolName[2],"的正確率,TPR=",round(TPR,2)))
TNR <- (cm[1]/sum(cm[1,]))
print(paste("預測",mycolName[2],"的正確率,TNR=",round(TNR,2)))
accuracy <- sum(diag(cm))/sum(cm)
print(paste("整體準確率 accuracy=",round(accuracy,2)))
}else if(length(cm)==2){
cm
precision <- (cm[[2]]/sum(cm[,1]))
print(paste("預測",mycolName[2],"的正確率,precision=",round(precision,2)))
TPR <- (cm[[2]]/sum(cm[2,]))
print(paste("預測",mycolName[2],"的正確率,TPR=",round(TPR,2)))
accuracy <- sum(cm[[2]]/sum(cm[,1]))
print(paste("整體準確率 accuracy=",round(accuracy,2)))
cm
}
Real <- as.factor(ifelse(testdata$status==1,"Y","N"))
Pred <- as.factor(ifelse(prediction==1,"Y","N"))
cmKNN <- table(Real=Real,Pred=Pred)
print(cmKNN)
cmKNN.factor <- table(factor(Real,ordered=TRUE,levels=c("Y","N")),factor(Pred,ordered=TRUE,levels=c("Y","N")),dnn=c("Real","Pred"))
print(cmKNN.factor)
if(length(cmKNN.factor)==4){
mycolName <- colnames(cmKNN.factor)
mycolName[1]
precision <- (cmKNN.factor[[1]]/sum(cmKNN.factor[,1]))
print(paste("預測",mycolName[1],"的正確率,precision=",round(precision,2)))
TPR <- (cmKNN.factor[[1]]/sum(cmKNN.factor[1,]))
print(paste("預測",mycolName[1],"的正確率,TPR=",round(TPR,2)))
TNR <- (cmKNN.factor[[4]]/sum(cmKNN.factor[2,]))
print(paste("預測",mycolName[1],"的正確率,TNR=",round(TNR,2)))
precision <- (cmKNN.factor[[4]]/sum(cmKNN.factor[,2]))
print(paste("預測",mycolName[2],"的正確率,precision=",round(precision,2)))
TPR <- (cmKNN.factor[[4]]/sum(cmKNN.factor[2,]))
print(paste("預測",mycolName[2],"的正確率,TPR=",round(TPR,2)))
TNR <- (cmKNN.factor[1]/sum(cmKNN.factor[1,]))
print(paste("預測",mycolName[2],"的正確率,TNR=",round(TNR,2)))
accuracy <- sum(diag(cmKNN.factor))/sum(cmKNN.factor)
print(paste("整體準確率=",round(accuracy,2)))
}else if(length(cmKNN.factor)==2){
precision <- (cmKNN.factor[[2]]/sum(cmKNN.factor[,1]))
paste("預測",mycolName[2],"的正確率,precision=",round(precision))
TPR <- (cmKNN.factor[[2]]/sum(cmKNN.factor[2,]))
print(paste("預測",mycolName[2],"的正確率,TPR=",round(TPR)))
accuracy <- sum(diag(cmKNN.factor)[[2]])/sum(cmKNN.factor[,1])
print(paste("整體準確率=",round(accuracy,2)))
}
print(cmKNN.factor)
klist <- seq(1:(kv+kv))
knnFunction <- function(x,knnTrain,knnTest,trainLabels,testLabels){
prediction <- knn(train = knnTrain,test=knnTest,cl=trainLabels,k=x)
cm <- table(x=testLables,y=prediction)
accuracy <- sum(diag(cm))/sum(cm)
}
accuracies <- sapple(klist,knnFunction,knnTrain=knnTrain,knnTest=knnTest,trainLabels=trainLabels,testLabels=testdata$status)
df <- data.frame(kv=klist,accuracy=accuracies)
accuracies <- sapple(klist,knnFunction,knnTrain=knnTrain,knnTest=knnTest,trainLabels=trainLabels,testLabels=testdata$status)
df <- data.frame(kv=klist,accuracy=accuracies)
install.packages("ggplot2")
library(ggplot2)
ggplot(df,aes(x=kv,y=accuracy,label=kv,color=accuracy))+geom-point(size=5)+geom_text(vjust=2)
version
install.packages("ggplot2")
source('D:/r place/knnForW10.R')
source('D:/r place/knnForW10.R')
source('D:/r place/knnForW10.R')
source('D:/r place/knnForW10.R')
source('D:/r place/knnForW10.R')
install.packages("ggplot2")
source('D:/r place/knnForW10.R')
install.packages("tidyr")
source('D:/r place/knnForW10.R')
install.packages("ggplot2")
source('D:/r place/knnForW10.R')
source('D:/r place/knnForW10.R')
source('D:/r place/knnForW10.R')
source('D:/r place/knnForW10.R')
source('D:/r place/knnForW10.R')
source('D:/r place/knnForW10.R')
source('D:/r place/knnForW10.R')
install.packages("tidyr")
source('D:/r place/knnForW10.R')
source('~/.active-rstudio-document', encoding = 'UTF-8')
source('~/.active-rstudio-document', encoding = 'UTF-8')
source('~/.active-rstudio-document', encoding = 'UTF-8')
source('~/.active-rstudio-document', encoding = 'UTF-8')
source('~/.active-rstudio-document', encoding = 'UTF-8')
source('~/.active-rstudio-document', encoding = 'UTF-8')
source('~/.active-rstudio-document', encoding = 'UTF-8')
source('~/.active-rstudio-document', encoding = 'UTF-8')
source('~/.active-rstudio-document', encoding = 'UTF-8')
source('~/.active-rstudio-document', encoding = 'UTF-8')
source('D:/r place/BayesianTheoryForW10.R', encoding = 'UTF-8')
source('D:/r place/BayesianTheoryForW10.R', encoding = 'UTF-8')
source('D:/r place/BayesianTheoryForW10.R', encoding = 'UTF-8')
source('D:/r place/BayesianTheoryForW10.R', encoding = 'UTF-8')
source('D:/r place/BayesianTheoryForW10.R', encoding = 'UTF-8')
source('D:/r place/BayesianTheoryForW10.R', encoding = 'UTF-8')
source('D:/r place/BayesianTheoryForW10.R', encoding = 'UTF-8')
source('D:/r place/BayesianTheoryForW10.R', encoding = 'UTF-8')
source('D:/r place/BayesianTheoryForW10.R', encoding = 'UTF-8')
source('D:/r place/BayesianTheoryForW10.R', encoding = 'UTF-8')
source('D:/r place/BayesianTheoryForW10.R', encoding = 'UTF-8')
source('D:/r place/BayesianTheoryForW10.R', encoding = 'UTF-8')
source('D:/r place/BayesianTheoryForW10.R', encoding = 'UTF-8')
source('D:/r place/BayesianTheoryForW10.R', encoding = 'UTF-8')
source('D:/r place/BayesianTheoryForW10.R', encoding = 'UTF-8')
source('D:/r place/BayesianTheoryForW10.R', encoding = 'UTF-8')
source('D:/r place/BayesianTheoryForW10.R', encoding = 'UTF-8')
source('D:/r place/BayesianTheoryForW10.R', encoding = 'UTF-8')
source('D:/r place/BayesianTheoryForW10.R', encoding = 'UTF-8')
source('D:/r place/BayesianTheoryForW10.R', encoding = 'UTF-8')
source('D:/r place/BayesianTheoryForW10.R', encoding = 'UTF-8')
source('D:/r place/BayesianTheoryForW10.R', encoding = 'UTF-8')
source('D:/r place/BayesianTheoryForW10.R', encoding = 'UTF-8')

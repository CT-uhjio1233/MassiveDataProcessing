<<<<<<< HEAD
accuracy <- sum(diag(cmKNN.factor)[[2]])/sum(cmKNN.factor[,1])
print(paste("整體準確率=",round(accuracy,2)))
}
print(cmKNN.factor)
klist <- seq(1:(kv+kv))
knnFunction <- function(x,knnTrain,knnTest,trainLabels,testLabels){
prediction <- knn(train = knnTrain,test=knnTest,cl=trainLabels,k=x)
cm <- table(x=testLables,y=prediction)
accuracy <- sum(diag(cm))/sum(cm)
}
accuracies <- sapple(klist,knnFunction,knnTrain=knnTrain,knnTest=knnTest,trainLabels=trainLabels,testLabels=testdata$status)
df <- data.frame(kv=klist,accuracy=accuracies)
install.packages("ggplot2")
library(ggplot2)
ggplot(df,aes(x=kv,y=accuracy,label=kv,color=accuracy))+geom-point(size=5)+geom_text(vjust=2)
version
version
source('~/Documents/database/knnRcode.R')
source('~/Documents/database/BayesianProbabilityForMAC.R')
source('~/Documents/database/BayesianProbabilityForMAC.R')
source('~/Documents/database/BayesianProbabilityForMAC.R')
source('~/Documents/database/BayesianProbabilityForMAC.R')
source('~/Documents/database/BayesianProbabilityForMAC.R')
source('~/Documents/database/BayesianProbabilityForMAC.R')
source('~/Documents/database/BayesianProbabilityForMAC.R')
source('~/Documents/database/BayesianProbabilityForMAC.R')
source('~/Documents/database/R_Code_ANN.R')
source('~/Documents/database/BayesianProbabilityForMAC.R')
source('~/Documents/database/knnRcode.R')
source('~/Documents/database/knnRcode.R')
source('~/Documents/database/knnRcode.R')
source('~/Documents/database/knnRcode.R')
source('~/Documents/database/knnRcode.R')
source('~/Documents/database/knnRcode.R')
source('~/Documents/database/knnRcode.R')
source('~/Documents/database/knnRcode.R')
source('~/Documents/database/knnRcode.R')
source('~/Documents/database/knnRcode.R')
source('~/Documents/database/knnRcode.R')
source('~/Documents/database/knnRcode.R')
library(class)
library(dplyr)
if(!(packageName%in%rownames(installed.packages()))){
install.packages(packageName)
}
setwd("~/Documents/database")
#測試模型,可隨機產生(訓練資料,測試資料)
traindata=read.csv("Parkinsons_Test.csv")
testdata=read.csv("Parkinsons_Test.csv")
=======
#建立混淆矩陣(confusion,matrix)觀察模型表現
cm <- table(testdata$status,result,dnn=c("實際","預測"))
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/DecisionTreeForWin10.R')
setwd("D:/r place")
#測試模型,可隨機產生(訓練資料,測試資料)
traindata=read.csv("wdbctrain.csv")
testdata=read.csv("wdbctest.csv")
require(rpart)
#建立決策樹模型;(因變數~自變數)
DataTree <- rpart(status~., data = traindata, method = "class")
#畫決策樹
require(rpart.plot)
prp(DataTree,          #模型
faclen=0,          #呈現的變數不要縮寫
fallen.leaves=TRUE,#讓樹枝以垂直的方式呈現
shadow.col="gray", #最下方的節點塗上陰影
extra=2)
#預測
result <- predict(DataTree,newdata=testdata[-20],type="class")
#建立混淆矩陣(confusion,matrix)觀察模型表現
cm <- table(testdata$status,result,dnn=c("實際"))
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/DecisionTreeForWin10.R')
#測試模型,可隨機產生(訓練資料,測試資料)
traindata=read.csv("wdbctrain.csv")
testdata=read.csv("wdbctest.csv")
print(traindata)
#建立決策樹模型;(因變數~自變數)
DataTree <- rpart(status~, data = traindata, method = "class")
testdata=read.csv("wdbctest.csv")
print(traindata)
require(rpart)
#建立決策樹模型;(因變數~自變數)
DataTree <- rpart(status~., data = traindata, method = "class")
#畫決策樹
require(rpart.plot)
prp(DataTree,          #模型
faclen=0,          #呈現的變數不要縮寫
fallen.leaves=TRUE,#讓樹枝以垂直的方式呈現
shadow.col="gray", #最下方的節點塗上陰影
extra=2)
fancyRpartPlot(DataTree)
#預測
result <- predict(DataTree,newdata=testdata,type="class")
#預測
print(DataTree)
result <- predict(DataTree,newdata=testdata,type="class")
#測試模型,可隨機產生(訓練資料,測試資料)
traindata=read.csv("wdbctrain.csv")
testdata=read.csv("wdbctest.csv")
print(traindata)
require(rpart)
#建立決策樹模型;(因變數~自變數)
DataTree <- rpart(status~., data = traindata, method = "class")
#建立決策樹模型;(因變數~自變數)
DataTree <- rpart(status~., data = traindata, method = "class")
print(status~.)
#畫決策樹
require(rpart.plot)
print(traindata)
require(rpart)
#建立決策樹模型;(因變數~自變數)
DataTree <- rpart(status~., data = traindata, method = "class")
print(rpart(status~.))
source('D:/r place/DecisionTreeForWin10.R')
result <- predict(DataTree,newdata=testdata,type="class")
#建立混淆矩陣(confusion,matrix)觀察模型表現
cm <- table(testdata$status,result,dnn=c("實際","預測"))
print(cm)
setwd("D:/r place")
#測試模型,可隨機產生(訓練資料,測試資料)
traindata=read.csv("wdbctrain.csv")
testdata=read.csv("wdbctest.csv")
print(traindata)
require(rpart)
#建立決策樹模型;(因變數~自變數)
DataTree <- rpart(status~., data = traindata, method = "class")
print(DataTree)
setwd("D:/r place")
#測試模型,可隨機產生(訓練資料,測試資料)
traindata=read.csv("wdbctrain.csv")
testdata=read.csv("wdbctest.csv")
require(rpart)
#建立決策樹模型;(因變數~自變數)
DataTree <- rpart(status~., data = traindata, method = "class")
print(DataTree)
#畫決策樹
require(rpart.plot)
prp(DataTree,          #模型
faclen=0,          #呈現的變數不要縮寫
fallen.leaves=TRUE,#讓樹枝以垂直的方式呈現
shadow.col="gray", #最下方的節點塗上陰影
extra=2)
fancyRpartPlot(DataTree)
result <- predict(DataTree,newdata=testdata,type="class")
result <- predict(DataTree,type="class")
#建立混淆矩陣(confusion,matrix)觀察模型表現
cm <- table(testdata$status,result,dnn=c("實際","預測"))
source('D:/r place/DecisionTreeForWin10.R')
result <- predict(DataTree,newdata=testdata,type="class")
#建立混淆矩陣(confusion,matrix)觀察模型表現
cm <- table(testdata$status,result,dnn=c("實際","預測"))
source('D:/r place/R_Code_ANNforWin10.R', encoding = 'UTF-8')
source('D:/r place/R_Code_ANNforWin10.R', encoding = 'UTF-8')
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/DecisionTreeForWin10.R')
source('D:/r place/R_Code_ANNforWin10.R', encoding = 'UTF-8')
source('D:/r place/R_Code_ANNforWin10.R', encoding = 'UTF-8')
source('D:/r place/R_Code_ANNforWin10.R', encoding = 'UTF-8')
source('D:/r place/R_Code_ANNforWin10.R', encoding = 'UTF-8')
source('D:/r place/R_Code_ANNforWin10.R', encoding = 'UTF-8')
source('D:/r place/R_Code_ANNforWin10.R', encoding = 'UTF-8')
source('D:/r place/NeuralNetworkForFinalReport.R', encoding = 'UTF-8')
source('D:/r place/NeuralNetworkForFinalReport.R', encoding = 'UTF-8')
source('D:/r place/NeuralNetworkForFinalReport.R', encoding = 'UTF-8')
source('D:/r place/NeuralNetworkForFinalReport.R', encoding = 'UTF-8')
source('D:/r place/NeuralNetworkForFinalReport.R', encoding = 'UTF-8')
source('D:/r place/NeuralNetworkForFinalReport.R', encoding = 'UTF-8')
source('D:/r place/NeuralNetworkForFinalReport.R', encoding = 'UTF-8')
source('D:/r place/NeuralNetworkForFinalReport.R', encoding = 'UTF-8')
source('D:/r place/NeuralNetworkForFinalReport.R', encoding = 'UTF-8')
source('D:/r place/NeuralNetworkForFinalReport.R', encoding = 'UTF-8')
source('D:/r place/NeuralNetworkForFinalReport.R', encoding = 'UTF-8')
source('D:/r place/NeuralNetworkForFinalReport.R', encoding = 'UTF-8')
source('D:/r place/NeuralNetworkForFinalReport.R', encoding = 'UTF-8')
source('D:/r place/NeuralNetworkForFinalReport.R', encoding = 'UTF-8')
source('D:/r place/NeuralNetworkForFinalReport.R', encoding = 'UTF-8')
source('D:/r place/NeuralNetworkForFinalReport.R', encoding = 'UTF-8')
source('D:/r place/NeuralNetworkForFinalReport.R', encoding = 'UTF-8')
source('D:/r place/NeuralNetworkForFinalReport.R', encoding = 'UTF-8')
source('D:/r place/NeuralNetworkForFinalReport.R', encoding = 'UTF-8')
source('D:/r place/NeuralNetworkForFinalReport.R', encoding = 'UTF-8')
source('D:/r place/NeuralNetworkForFinalReport.R', encoding = 'UTF-8')
source('D:/r place/NeuralNetworkForFinalReport.R', encoding = 'UTF-8')
source('D:/r place/NeuralNetworkForFinalReport.R', encoding = 'UTF-8')
source('D:/r place/NeuralNetworkForFinalReport.R', encoding = 'UTF-8')
source('D:/r place/NeuralNetworkForFinalReport.R', encoding = 'UTF-8')
source('D:/r place/NeuralNetworkForFinalReport.R', encoding = 'UTF-8')
source('D:/r place/NeuralNetworkForFinalReport.R', encoding = 'UTF-8')
source('D:/r place/NeuralNetworkForFinalReport.R', encoding = 'UTF-8')
source('D:/r place/NeuralNetworkForFinalReport.R', encoding = 'UTF-8')
source('D:/r place/NeuralNetworkForFinalReport.R', encoding = 'UTF-8')
source('D:/r place/NeuralNetworkForFinalReport.R', encoding = 'UTF-8')
source('D:/r place/NeuralNetworkForFinalReport.R', encoding = 'UTF-8')
source('D:/r place/NeuralNetworkForFinalReport.R', encoding = 'UTF-8')
source('D:/r place/NeuralNetworkForFinalReport.R', encoding = 'UTF-8')
source('D:/r place/NeuralNetworkForFinalReport.R', encoding = 'UTF-8')
require(neuralnet)#for neuralnet(),nn model
require(nnet)#for class.ind()
require(caret)#for train(),tune paramenters
setwd("D:/r place")#設定工作目錄
traindata=read.csv("LR_train.csv")
testdata=read.csv("LR_test.csv")
testdata <- testdata[-c(1)]
traindata <- traindata[-c(1)]
#原始資料就會變成像這樣
head(traindata)
formula.bpn <- as.formula(status ~ x-box+y-box+width+high+onpix+x-bar+y-bar+x2bar+y2bar+xybar+x2ybr+xy2br+x-ege+xegvy+y-ege+yegvy)
bpn <- neuralnet(data=traindata,
formula=formula.bpn,
hidden=c(2),#一個隱藏層:2個node
learningrate=0.01,#leearing rate
threshold=0.01,#partial derivatives of the error function, a stopping criteria
stepmax=5e5#最大的iteration數=500000(5*10^5)
)
plot(bpn)
pred<- compute(bpn,testdata[1:22])
pred$net.result
#pred.result<-round(pre$net.result)
pred.result<-round(1/(1+exp(-pred$net.result)))
pred.result
cm <- table(testdata$status,pred.result)
print(cm)
if(length(cm)==4){
mycolNames<-colnames(cm)
mycolName[1]
precision <- (cm[[1]]/sum(cm[,1]))
paste("預測",mycolName[1],"的正確率,precision=",precision)
TPR <- (cm[[1]]/sum(cm[1,]))
print(paste("預測",mycolName[1],"的正確率,TPR=",TPR))
TNR <- (cm[[4]]/sum(cm[2,]))
print(paste("預測",mycolName[1],"的正確率,TNR=",TNR))
precision <- (cm[[4]]/sum(cm[,2]))
paste("預測",mycolName[1],"的正確率,precision=",precision)
TPR <- (cm[[4]]/sum(cm[2,]))
print(paste("預測",mycolName[1],"的正確率,TPR=",TPR))
TNR <- (cm[[1]]/sum(cm[1,]))
print(paste("預測",mycolName[1],"的正確率,TNR=",TNR))
accuracy <- sum(diag(cm))/sum(cm)
print(paste("整體準確率=",round(accuracy,2)))
}else{
print("confusuion matrix 的個數<4,需額外計算")
}
source('D:/r place/NeuralNetworkForFinalReport.R', encoding = 'UTF-8')
source('D:/r place/NeuralNetworkForFinalReport.R', encoding = 'UTF-8')
source('D:/r place/NeuralNetworkForFinalReport.R', encoding = 'UTF-8')
require(neuralnet)#for neuralnet(),nn model
require(nnet)#for class.ind()
require(caret)#for train(),tune paramenters
setwd("D:/r place")#設定工作目錄
traindata=read.csv("LR_train.csv")
testdata=read.csv("LR_test.csv")
testdata <- testdata[-c(1)]
traindata <- traindata[-c(1)]
#原始資料就會變成像這樣
head(traindata)
formula.bpn <- as.formula(paste("status ~",paste( xbox+ybox+width+high+onpix+x-bar+y-bar+x2bar+y2bar+xybar+x2ybr+xy2br+x-ege+xegvy+y-ege+yegvy)))
bpn <- neuralnet(data=traindata,
formula=formula.bpn,
hidden=c(2),#一個隱藏層:2個node
learningrate=0.01,#leearing rate
threshold=0.01,#partial derivatives of the error function, a stopping criteria
stepmax=5e5#最大的iteration數=500000(5*10^5)
)
plot(bpn)
pred<- compute(bpn,testdata[1:22])
pred$net.result
#pred.result<-round(pre$net.result)
pred.result<-round(1/(1+exp(-pred$net.result)))
pred.result
cm <- table(testdata$status,pred.result)
print(cm)
if(length(cm)==4){
mycolNames<-colnames(cm)
mycolName[1]
precision <- (cm[[1]]/sum(cm[,1]))
print(paste("預測",mycolName[1],"的正確率,precision=",precision))
TPR <- (cm[[1]]/sum(cm[1,]))
print(paste("預測",mycolName[1],"的正確率,TPR=",TPR))
TNR <- (cm[[4]]/sum(cm[2,]))
print(paste("預測",mycolName[1],"的正確率,TNR=",TNR))
precision <- (cm[[4]]/sum(cm[,2]))
paste("預測",mycolName[1],"的正確率,precision=",precision)
TPR <- (cm[[4]]/sum(cm[2,]))
print(paste("預測",mycolName[1],"的正確率,TPR=",TPR))
TNR <- (cm[[1]]/sum(cm[1,]))
print(paste("預測",mycolName[1],"的正確率,TNR=",TNR))
accuracy <- sum(diag(cm))/sum(cm)
print(paste("整體準確率=",round(accuracy,2)))
}else{
print("confusuion matrix 的個數<4,需額外計算")
}
source('D:/r place/NeuralNetworkForFinalReport.R', encoding = 'UTF-8')
source('D:/r place/NeuralNetworkForFinalReport.R', encoding = 'UTF-8')
source('D:/r place/NeuralNetworkForFinalReport.R', encoding = 'UTF-8')
source('D:/r place/NeuralNetworkForFinalReport.R', encoding = 'UTF-8')
source('D:/r place/NeuralNetworkForFinalReport.R', encoding = 'UTF-8')
source('D:/r place/R_Code_ANNforWin10.R')
source('D:/r place/NeuralNetworkForFinalReport.R', encoding = 'UTF-8')
source('D:/r place/NeuralNetworkForFinalReport.R', encoding = 'UTF-8')
source('D:/r place/R_Code_ANNforWin10.R')
require(neuralnet)#for neuralnet(),nn model
require(nnet)#for class.ind()
require(caret)#for train(),tune paramenters
setwd("D:/r place")#設定工作目錄
traindata=read.csv("LR_train.csv")
testdata=read.csv("LR_test.csv")
testdata <- testdata[-c(1)]
traindata <- traindata[-c(1)]
#原始資料就會變成像這樣
head(traindata)
formula.bpn <- as.formula(status ~ xbox+ybox+width+high+onpix+xbar+ybar+x2bar+y2bar+xybar+x2ybr+xy2br+x-ege+xegvy+y-ege+yegvy)
bpn <- neuralnet(data=traindata,
formula=formula.bpn,
hidden=c(2),#一個隱藏層:2個node
learningrate=0.01,#leearing rate
threshold=0.01,#partial derivatives of the error function, a stopping criteria
stepmax=5e5#最大的iteration數=500000(5*10^5)
)
plot(bpn)
require(neuralnet)#for neuralnet(),nn model
require(nnet)#for class.ind()
require(caret)#for train(),tune paramenters
setwd("D:/r place")#設定工作目錄
traindata=read.csv("LR_train.csv")
testdata=read.csv("LR_test.csv")
testdata <- testdata[-c(1)]
traindata <- traindata[-c(1)]
#原始資料就會變成像這樣
head(traindata)
formula.bpn <- as.formula(status ~ xbox+ybox+width+high+onpix+xbar+ybar+x2bar+y2bar+xybar+x2ybr+xy2br+x-ege+xegvy+y-ege+yegvy)
bpn <- neuralnet(data=traindata,
formula=formula.bpn,
hidden=c(2),#一個隱藏層:2個node
learningrate=0.01,#leearing rate
threshold=0.01,#partial derivatives of the error function, a stopping criteria
stepmax=5e5#最大的iteration數=500000(5*10^5)
)
plot(bpn)
pred<- compute(bpn,testdata[1:22])
pred$net.result
require(neuralnet)#for neuralnet(),nn model
require(nnet)#for class.ind()
require(caret)#for train(),tune paramenters
setwd("D:/r place")#設定工作目錄
traindata=read.csv("LR_train.csv")
testdata=read.csv("LR_test.csv")
testdata <- testdata[-c(1)]
traindata <- traindata[-c(1)]
#原始資料就會變成像這樣
head(traindata)
formula.bpn <- as.formula(status ~ xbox+ybox+width+high+onpix+xbar+ybar+x2bar+y2bar+xybar+x2ybr+xy2br+x-ege+xegvy+y-ege+yegvy)
bpn <- neuralnet(data=traindata,
formula=formula.bpn,
hidden=c(2),#一個隱藏層:2個node
learningrate=0.01,#leearing rate
threshold=0.01,#partial derivatives of the error function, a stopping criteria
stepmax=5e5#最大的iteration數=500000(5*10^5)
)
plot(bpn)
source('D:/r place/R_Code_ANNforWin10.R')
source('D:/r place/knnForW10.R')
source('D:/r place/knnForW10.R')
source('D:/r place/knnForW10.R')
source('D:/r place/knnForW10.R')
install.packages(c("ellipsis", "glue", "rlang", "tidyr", "vctrs"))
install.packages(c("ellipsis", "glue", "rlang", "tidyr", "vctrs"))
install.packages("dplyr")
if(!(packageName%in%rownames(installed.packages()))){
install.packages(packageName)
}
library(class)
library(dplyr)
setwd("D:/r place")
#測試模型,可隨機產生(訓練資料,測試資料)
traindata=read.csv("Parkinsons_TestANN.csv")
testdata=read.csv("Parkinsons_TestANN.csv")
>>>>>>> d0963bb96b542fcf5479ec77e110b70426c3691d
#去除第一欄PK不適合分岔屬性
testdata <- testdata[-c(1)]
traindata <- traindata[-c(1)]
trainLabels <- traindata$status
knnTrain <- traindata[,-c(24)]
knnTest <- testdata[,-c(24)]
kv <- round(sqrt(length(knnTrain)))
prediction <- knn(train = knnTrain, test = knnTest, cl = trainLabels, k=kv)
cm <- table(testdata$status,prediction,dnn=c("實際","預測"))
print(cm)
if(length(cm)==4){
nycolName <- colnames(cm)
mycolName[1]
precision <- (cm[[1]]/sum(cm[,1]))
print(paste("預測",mycolName[1],"的正確率,precision=",round(precision,2)))
TPR <- (cm[[1]]/sum(cm[1,]))
print(paste("預測",mycolName[1],"的正確率,TPR=",round(TPR,2)))
TNR <- (cm[[4]]/sum(cm[2,]))
print(paste("預測",mycolName[1],"的正確率,TNR=",round(TNR,2)))
precision <- (cm[[4]]/sum(cm[,2]))
print(paste("預測",mycolName[2],"的正確率,precision=",round(precision,2)))
TPR <- (cm[[4]]/sum(cm[2,]))
print(paste("預測",mycolName[2],"的正確率,TPR=",round(TPR,2)))
TNR <- (cm[1]/sum(cm[1,]))
print(paste("預測",mycolName[2],"的正確率,TNR=",round(TNR,2)))
accuracy <- sum(diag(cm))/sum(cm)
print(paste("整體準確率 accuracy=",round(accuracy,2)))
}else if(length(cm)==2){
cm
precision <- (cm[[2]]/sum(cm[,1]))
print(paste("預測",mycolName[2],"的正確率,precision=",round(precision,2)))
TPR <- (cm[[2]]/sum(cm[2,]))
print(paste("預測",mycolName[2],"的正確率,TPR=",round(TPR,2)))
accuracy <- sum(cm[[2]]/sum(cm[,1]))
print(paste("整體準確率 accuracy=",round(accuracy,2)))
cm
}
Real <- as.factor(ifelse(testdata$status==1,"Y","N"))
Pred <- as.factor(ifelse(prediction==1,"Y","N"))
cmKNN <- table(Real=Real,Pred=Pred)
print(cmKNN)
cmKNN.factor <- table(factor(Real,ordered=TRUE,levels=c("Y","N")),factor(Pred,ordered=TRUE,levels=c("Y","N")),dnn=c("Real","Pred"))
print(cmKNN.factor)
if(length(cmKNN.factor)==4){
mycolName <- colnames(cmKNN.factor)
mycolName[1]
precision <- (cmKNN.factor[[1]]/sum(cmKNN.factor[,1]))
print(paste("預測",mycolName[1],"的正確率,precision=",round(precision,2)))
TPR <- (cmKNN.factor[[1]]/sum(cmKNN.factor[1,]))
print(paste("預測",mycolName[1],"的正確率,TPR=",round(TPR,2)))
TNR <- (cmKNN.factor[[4]]/sum(cmKNN.factor[2,]))
print(paste("預測",mycolName[1],"的正確率,TNR=",round(TNR,2)))
precision <- (cmKNN.factor[[4]]/sum(cmKNN.factor[,2]))
print(paste("預測",mycolName[2],"的正確率,precision=",round(precision,2)))
TPR <- (cmKNN.factor[[4]]/sum(cmKNN.factor[2,]))
print(paste("預測",mycolName[2],"的正確率,TPR=",round(TPR,2)))
TNR <- (cmKNN.factor[1]/sum(cmKNN.factor[1,]))
print(paste("預測",mycolName[2],"的正確率,TNR=",round(TNR,2)))
accuracy <- sum(diag(cmKNN.factor))/sum(cmKNN.factor)
print(paste("整體準確率=",round(accuracy,2)))
}else if(length(cmKNN.factor)==2){
precision <- (cmKNN.factor[[2]]/sum(cmKNN.factor[,1]))
paste("預測",mycolName[2],"的正確率,precision=",round(precision))
TPR <- (cmKNN.factor[[2]]/sum(cmKNN.factor[2,]))
print(paste("預測",mycolName[2],"的正確率,TPR=",round(TPR)))
accuracy <- sum(diag(cmKNN.factor)[[2]])/sum(cmKNN.factor[,1])
print(paste("整體準確率=",round(accuracy,2)))
}
print(cmKNN.factor)
klist <- seq(1:(kv+kv))
knnFunction <- function(x,knnTrain,knnTest,trainLabels,testLabels){
prediction <- knn(train = knnTrain,test=knnTest,cl=trainLabels,k=x)
cm <- table(x=testLables,y=prediction)
accuracy <- sum(diag(cm))/sum(cm)
}
accuracies <- sapple(klist,knnFunction,knnTrain=knnTrain,knnTest=knnTest,trainLabels=trainLabels,testLabels=testdata$status)
df <- data.frame(kv=klist,accuracy=accuracies)
<<<<<<< HEAD
install.packages("ggplot2")
library(ggplot2)
ggplot(df,aes(x=kv,y=accuracy,label=kv,color=accuracy))+geom-point(size=5)+geom_text(vjust=2)
source('~/Documents/database/BayesianProbabilityForMAC.R')
source('~/Documents/database/BayesianProbabilityForMAC.R')
source('~/Documents/database/BayesianProbabilityForMAC.R')
source('~/Documents/database/BayesianProbabilityForMAC.R')
source('~/Documents/database/BayesianProbabilityForMAC.R')
source('~/Documents/database/BayesianProbabilityForMAC.R')
ratio
plot(formula=Petal.Length~Petal.Width,data=iris,col=iris$Species)
packageName="ggplot"
if(!(packageName %in% rownames(installed.packages()))){
install.packages(packageName)
}
data<-iris[-5]
km<-kmeans(data,centers=3,nstart = 10)
plot(formula =Petal.Length~Petal.Width,data=data,col=km$cluster,main="鳶尾花(iris)分群",xlab="Petal.Width(花瓣寬度)",
ylab="Petal.Length(花瓣長度)")
library("ggplot2")
ggplot(data,aes(x=Petal.Length,y=Petal.Width))+geom_point(aes(x=Petal.Length,y=Petal.Width))+
geom_point(aes(color=factor(km$cluster)))
WSS<-km$tot.withinss
WSS
BSS<- km$betweenss
BSS
TSS<- BSS+WSS
TSS
ratio<-WSS/TSS
ratio
Accuracy.DF=data.frame(Name=c("KNN","RF","NaiveBay","SVM","Dtree","ANN"))
Accuracy.DF$Accuracy=c(knnaccuracy,RFaccuracy,NBaccuracy,SVMaccuracy,DTreeaccuracy,ANNaccuracy)
Accuracy.DF
arrange(Accuracy.DF,desc(Accuracy))
df<- data.frame(Accuracy = c(knnaccuracy,RFaccuracy,NBaccuracy,SVMaccuracy,DTreeaccuracy,ANNaccuracy),
name=c("KNN","RF","NaiveBay","SVM","DTree","ANN"))
ggplot(df,aes(x=name,y=Accuracy,color=name,label=Accuracy))+geom_point(size=5)+geom_text(vjust=2)
Accuracy.DF=data.frame(Name=c("KNN","RF","NaiveBay","SVM","Dtree","ANN"))
Accuracy.DF$Accuracy=c(knnaccuracy,RFaccuracy,NBaccuracy,SVMaccuracy,DTreeaccuracy,ANNaccuracy)
Accuracy.DF
arrange(Accuracy.DF,desc(Accuracy))
df<- data.frame(Accuracy = c(knnaccuracy,RFaccuracy,NBaccuracy,SVMaccuracy,DTreeaccuracy,ANNaccuracy),
name=c("KNN","RF","NaiveBay","SVM","DTree","ANN"))
ggplot(df,aes(x=name,y=Accuracy,color=name,label=Accuracy))+geom_point(size=5)+geom_text(vjust=2)
plot(formula=Petal.Length~Petal.Width,data=iris,col=iris$Species)
packageName="ggplot"
if(!(packageName %in% rownames(installed.packages()))){
install.packages(packageName)
}
data<-iris[-5]
km<-kmeans(data,centers=3,nstart = 10)
plot(formula =Petal.Length~Petal.Width,data=data,col=km$cluster,main="鳶尾花(iris)分群",xlab="Petal.Width(花瓣寬度)",
ylab="Petal.Length(花瓣長度)")
library("ggplot2")
ggplot(data,aes(x=Petal.Length,y=Petal.Width))+geom_point(aes(x=Petal.Length,y=Petal.Width))+
geom_point(aes(color=factor(km$cluster)))
WSS<-km$tot.withinss
WSS
BSS<- km$betweenss
BSS
TSS<- BSS+WSS
TSS
ratio<-WSS/TSS
ratio
WSS
setwd('D:/TestData')
traindata=read.csv("Parkinsons_TrainANN.csv")
testdata=read.csv("Parkinsons_TestANN.csv")
testdata<-testdata[-c(1)]
traindata<-traindata[-c(1)]
packageName="e1071"
if(!(packageName %in%rownames(installed.packages()))){
install.packages(packageName)
}
library(e1071)
svmM<-svm(status~.,data=traindata,probability=TRUE)
results<-predict(svmM,testdata,probability=TRUE)
result.TR<-round(1/(1+exp(-results)))
Real<-as.factor(ifelse(testdata$status==1,"Y","N"))
Pred<-as.factor(ifelse(result.TR==1,"Y","N"))
cm<-table(R=Real,P=Pred)
cm
cmSVM<-table(Real=Real,Pred=Pred)
cmSVM
cmSVM.factor<-table(factor(Real,ordered=TRUE,levels=c("Y","N")),factor(Pred,order=TRUE,levels=c("Y","N")),dnn=c("Real","Pred"))
cmSVM.factor
SVMaccuracy<-round(sum(diag(cmSVM.factor))/sum(cmSVM.factor),4)
SVMaccuracy
if(length(cmSVM.factor)==4){
mycolName<-colnames(cmSVM.factor)
mycolName[1]
precision<-(cmSVM.factor[[1]]/sum(cmSVM.factor[,1]))
print(paste("預測",mycolName[1],"的正確率，precision=",round(precision,2)))
if(length(cmSVM.factor)==4){
mycolNames<-colnames(cmSVM.factor)
mycolName[1]
precision <- (cmSVM.factor[[1]]/sum(cmSVM.factor[,1]))
paste("預測",mycolName[1],"的正確率,precision=",precision)
TPR <- (cmSVM.factor[[1]]/sum(cmSVM.factor[1,]))
print(paste("預測",mycolName[1],"的正確率,TPR=",TPR))
TNR <- (cmSVM.factor[[4]]/sum(cmSVM.factor[2,]))
print(paste("預測",mycolName[1],"的正確率,TNR=",TNR))
precision <- (cmSVM.factor[[4]]/sum(cmSVM.factor[,2]))
paste("預測",mycolName[1],"的正確率,precision=",precision)
TPR <- (cmSVM.factor[[4]]/sum(cmSVM.factor[2,]))
print(paste("預測",mycolName[1],"的正確率,TPR=",TPR))
TNR <- (cmSVM.factor[[1]]/sum(cmSVM.factor[1,]))
print(paste("預測",mycolName[1],"的正確率,TNR=",TNR))
accuracy <- sum(diag(cmSVM.factor))/sum(cmSVM.factor)
print(paste("整體準確率=",round(accuracy,2)))
}else if(length(cmSVM.factor)==2){
cm.factor
precision <- (cmSVM.factor[[2]]/sum(cmSVM.factor[,1]))
print(paste("預測",mycolName[2],"的正確率,precision=",round(precision,2)))
TPR <- (cmSVM.factor[[2]]/sum(cmSVM.factor[2,]))
print(paste("預測",mycolName[2],"的正確率,TPR=",round(TPR,2)))
accuracy <- sum(diag(cmSVM.factor)[[2]])/sum(cmSVM.factor[,1])
print(paste("整體準確率=",round(accuracy,2)))
}
cmSVM.factor
}
setwd('~/Documents/database')
traindata=read.csv("Parkinsons_TrainANN.csv")
testdata=read.csv("Parkinsons_TestANN.csv")
testdata<-testdata[-c(1)]
traindata<-traindata[-c(1)]
packageName="e1071"
if(!(packageName %in%rownames(installed.packages()))){
install.packages(packageName)
}
library(e1071)
svmM<-svm(status~.,data=traindata,probability=TRUE)
results<-predict(svmM,testdata,probability=TRUE)
result.TR<-round(1/(1+exp(-results)))
Real<-as.factor(ifelse(testdata$status==1,"Y","N"))
Pred<-as.factor(ifelse(result.TR==1,"Y","N"))
cm<-table(R=Real,P=Pred)
cm
cmSVM<-table(Real=Real,Pred=Pred)
cmSVM
cmSVM.factor<-table(factor(Real,ordered=TRUE,levels=c("Y","N")),factor(Pred,order=TRUE,levels=c("Y","N")),dnn=c("Real","Pred"))
cmSVM.factor
SVMaccuracy<-round(sum(diag(cmSVM.factor))/sum(cmSVM.factor),4)
SVMaccuracy
if(length(cmSVM.factor)==4){
mycolName<-colnames(cmSVM.factor)
mycolName[1]
precision<-(cmSVM.factor[[1]]/sum(cmSVM.factor[,1]))
print(paste("預測",mycolName[1],"的正確率，precision=",round(precision,2)))
if(length(cmSVM.factor)==4){
mycolNames<-colnames(cmSVM.factor)
mycolName[1]
precision <- (cmSVM.factor[[1]]/sum(cmSVM.factor[,1]))
paste("預測",mycolName[1],"的正確率,precision=",precision)
TPR <- (cmSVM.factor[[1]]/sum(cmSVM.factor[1,]))
print(paste("預測",mycolName[1],"的正確率,TPR=",TPR))
TNR <- (cmSVM.factor[[4]]/sum(cmSVM.factor[2,]))
print(paste("預測",mycolName[1],"的正確率,TNR=",TNR))
precision <- (cmSVM.factor[[4]]/sum(cmSVM.factor[,2]))
paste("預測",mycolName[1],"的正確率,precision=",precision)
TPR <- (cmSVM.factor[[4]]/sum(cmSVM.factor[2,]))
print(paste("預測",mycolName[1],"的正確率,TPR=",TPR))
TNR <- (cmSVM.factor[[1]]/sum(cmSVM.factor[1,]))
print(paste("預測",mycolName[1],"的正確率,TNR=",TNR))
accuracy <- sum(diag(cmSVM.factor))/sum(cmSVM.factor)
print(paste("整體準確率=",round(accuracy,2)))
}else if(length(cmSVM.factor)==2){
cm.factor
precision <- (cmSVM.factor[[2]]/sum(cmSVM.factor[,1]))
print(paste("預測",mycolName[2],"的正確率,precision=",round(precision,2)))
TPR <- (cmSVM.factor[[2]]/sum(cmSVM.factor[2,]))
print(paste("預測",mycolName[2],"的正確率,TPR=",round(TPR,2)))
accuracy <- sum(diag(cmSVM.factor)[[2]])/sum(cmSVM.factor[,1])
print(paste("整體準確率=",round(accuracy,2)))
}
cmSVM.factor
}
setwd('~/Documents/database')
traindata=read.csv("Parkinsons_TrainANN.csv")
testdata=read.csv("Parkinsons_TestANN.csv")
testdata<-testdata[-c(1)]
traindata<-traindata[-c(1)]
packageName="e1071"
if(!(packageName %in%rownames(installed.packages()))){
install.packages(packageName)
}
library(e1071)
svmM<-svm(status~.,data=traindata,probability=TRUE)
results<-predict(svmM,testdata,probability=TRUE)
result.TR<-round(1/(1+exp(-results)))
Real<-as.factor(ifelse(testdata$status==1,"Y","N"))
Pred<-as.factor(ifelse(result.TR==1,"Y","N"))
cm<-table(R=Real,P=Pred)
cm
cmSVM<-table(Real=Real,Pred=Pred)
cmSVM
cmSVM.factor<-table(factor(Real,ordered=TRUE,levels=c("Y","N")),factor(Pred,order=TRUE,levels=c("Y","N")),dnn=c("Real","Pred"))
cmSVM.factor
SVMaccuracy<-round(sum(diag(cmSVM.factor))/sum(cmSVM.factor),4)
SVMaccuracy
if(length(cmSVM.factor)==4){
mycolName<-colnames(cmSVM.factor)
mycolName[1]
precision<-(cmSVM.factor[[1]]/sum(cmSVM.factor[,1]))
print(paste("預測",mycolName[1],"的正確率，precision=",round(precision,2)))
if(length(cmSVM.factor)==4){
mycolNames<-colnames(cmSVM.factor)
mycolName[1]
precision <- (cmSVM.factor[[1]]/sum(cmSVM.factor[,1]))
paste("預測",mycolName[1],"的正確率,precision=",precision)
TPR <- (cmSVM.factor[[1]]/sum(cmSVM.factor[1,]))
print(paste("預測",mycolName[1],"的正確率,TPR=",TPR))
TNR <- (cmSVM.factor[[4]]/sum(cmSVM.factor[2,]))
print(paste("預測",mycolName[1],"的正確率,TNR=",TNR))
precision <- (cmSVM.factor[[4]]/sum(cmSVM.factor[,2]))
paste("預測",mycolName[1],"的正確率,precision=",precision)
TPR <- (cmSVM.factor[[4]]/sum(cmSVM.factor[2,]))
print(paste("預測",mycolName[1],"的正確率,TPR=",TPR))
TNR <- (cmSVM.factor[[1]]/sum(cmSVM.factor[1,]))
print(paste("預測",mycolName[1],"的正確率,TNR=",TNR))
accuracy <- sum(diag(cmSVM.factor))/sum(cmSVM.factor)
print(paste("整體準確率=",round(accuracy,2)))
}else if(length(cmSVM.factor)==2){
cm.factor
precision <- (cmSVM.factor[[2]]/sum(cmSVM.factor[,1]))
print(paste("預測",mycolName[2],"的正確率,precision=",round(precision,2)))
TPR <- (cmSVM.factor[[2]]/sum(cmSVM.factor[2,]))
print(paste("預測",mycolName[2],"的正確率,TPR=",round(TPR,2)))
accuracy <- sum(diag(cmSVM.factor)[[2]])/sum(cmSVM.factor[,1])
print(paste("整體準確率=",round(accuracy,2)))
}
cmSVM.factor
}
setwd('~/Documents/database')
traindata=read.csv("Parkinsons_TrainANN.csv")
testdata=read.csv("Parkinsons_TestANN.csv")
testdata<-testdata[-c(1)]
traindata<-traindata[-c(1)]
packageName="e1071"
if(!(packageName %in%rownames(installed.packages()))){
install.packages(packageName)
}
library(e1071)
svmM<-svm(status~.,data=traindata,probability=TRUE)
results<-predict(svmM,testdata,probability=TRUE)
result.TR<-round(1/(1+exp(-results)))
Real<-as.factor(ifelse(testdata$status==1,"Y","N"))
Pred<-as.factor(ifelse(result.TR==1,"Y","N"))
cm<-table(R=Real,P=Pred)
cm
cmSVM<-table(Real=Real,Pred=Pred)
cmSVM
cmSVM.factor<-table(factor(Real,ordered=TRUE,levels=c("Y","N")),factor(Pred,order=TRUE,levels=c("Y","N")),dnn=c("Real","Pred"))
cmSVM.factor
SVMaccuracy<-round(sum(diag(cmSVM.factor))/sum(cmSVM.factor),4)
SVMaccuracy
if(length(cmSVM.factor)==4){
mycolName<-colnames(cmSVM.factor)
mycolName[1]
precision<-(cmSVM.factor[[1]]/sum(cmSVM.factor[,1]))
print(paste("預測",mycolName[1],"的正確率，precision=",round(precision,2)))
if(length(cmSVM.factor)==4){
mycolNames<-colnames(cmSVM.factor)
mycolName[1]
precision <- (cmSVM.factor[[1]]/sum(cmSVM.factor[,1]))
paste("預測",mycolName[1],"的正確率,precision=",precision)
TPR <- (cmSVM.factor[[1]]/sum(cmSVM.factor[1,]))
print(paste("預測",mycolName[1],"的正確率,TPR=",TPR))
TNR <- (cmSVM.factor[[4]]/sum(cmSVM.factor[2,]))
print(paste("預測",mycolName[1],"的正確率,TNR=",TNR))
precision <- (cmSVM.factor[[4]]/sum(cmSVM.factor[,2]))
paste("預測",mycolName[1],"的正確率,precision=",precision)
TPR <- (cmSVM.factor[[4]]/sum(cmSVM.factor[2,]))
print(paste("預測",mycolName[1],"的正確率,TPR=",TPR))
TNR <- (cmSVM.factor[[1]]/sum(cmSVM.factor[1,]))
print(paste("預測",mycolName[1],"的正確率,TNR=",TNR))
accuracy <- sum(diag(cmSVM.factor))/sum(cmSVM.factor)
print(paste("整體準確率=",round(accuracy,2)))
}else if(length(cmSVM.factor)==2){
cm.factor
precision <- (cmSVM.factor[[2]]/sum(cmSVM.factor[,1]))
print(paste("預測",mycolName[2],"的正確率,precision=",round(precision,2)))
TPR <- (cmSVM.factor[[2]]/sum(cmSVM.factor[2,]))
print(paste("預測",mycolName[2],"的正確率,TPR=",round(TPR,2)))
accuracy <- sum(diag(cmSVM.factor)[[2]])/sum(cmSVM.factor[,1])
print(paste("整體準確率=",round(accuracy,2)))
}
cmSVM.factor
}
source('~/Documents/database/k-meansForMac.R')
plot(formula=Petal.Length~Petal.Width,data=iris,col=iris$Species)
packageName="ggplot"
if(!(packageName %in% rownames(installed.packages()))){
install.packages(packageName)
}
data<-iris[-5]
km<-kmeans(data,centers=3,nstart = 10)
plot(formula =Petal.Length~Petal.Width,data=data,col=km$cluster,main="鳶尾花(iris)分群",xlab="Petal.Width(花瓣寬度)",
ylab="Petal.Length(花瓣長度)")
library("ggplot2")
ggplot(data,aes(x=Petal.Length,y=Petal.Width))+geom_point(aes(x=Petal.Length,y=Petal.Width))+
geom_point(aes(color=factor(km$cluster)))
WSS<-km$tot.withinss
WSS
BSS<- km$betweenss
BSS
TSS<- BSS+WSS
TSS
ratio<-WSS/TSS
ratio
setwd('~/Documents/database')
traindata=read.csv("Parkinsons_TrainANN.csv")
testdata=read.csv("Parkinsons_TestANN.csv")
testdata<-testdata[-c(1)]
traindata<-traindata[-c(1)]
packageName="e1071"
if(!(packageName %in%rownames(installed.packages()))){
install.packages(packageName)
}
library(e1071)
svmM<-svm(status~.,data=traindata,probability=TRUE)
results<-predict(svmM,testdata,probability=TRUE)
result.TR<-round(1/(1+exp(-results)))
Real<-as.factor(ifelse(testdata$status==1,"Y","N"))
Pred<-as.factor(ifelse(result.TR==1,"Y","N"))
cm<-table(R=Real,P=Pred)
cm
cmSVM<-table(Real=Real,Pred=Pred)
cmSVM
cmSVM.factor<-table(factor(Real,ordered=TRUE,levels=c("Y","N")),factor(Pred,order=TRUE,levels=c("Y","N")),dnn=c("Real","Pred"))
cmSVM.factor
SVMaccuracy<-round(sum(diag(cmSVM.factor))/sum(cmSVM.factor),4)
SVMaccuracy
if(length(cmSVM.factor)==4){
mycolName<-colnames(cmSVM.factor)
mycolName[1]
precision<-(cmSVM.factor[[1]]/sum(cmSVM.factor[,1]))
print(paste("預測",mycolName[1],"的正確率，precision=",round(precision,2)))
if(length(cmSVM.factor)==4){
mycolNames<-colnames(cmSVM.factor)
mycolName[1]
precision <- (cmSVM.factor[[1]]/sum(cmSVM.factor[,1]))
paste("預測",mycolName[1],"的正確率,precision=",precision)
TPR <- (cmSVM.factor[[1]]/sum(cmSVM.factor[1,]))
print(paste("預測",mycolName[1],"的正確率,TPR=",TPR))
TNR <- (cmSVM.factor[[4]]/sum(cmSVM.factor[2,]))
print(paste("預測",mycolName[1],"的正確率,TNR=",TNR))
precision <- (cmSVM.factor[[4]]/sum(cmSVM.factor[,2]))
paste("預測",mycolName[1],"的正確率,precision=",precision)
TPR <- (cmSVM.factor[[4]]/sum(cmSVM.factor[2,]))
print(paste("預測",mycolName[1],"的正確率,TPR=",TPR))
TNR <- (cmSVM.factor[[1]]/sum(cmSVM.factor[1,]))
print(paste("預測",mycolName[1],"的正確率,TNR=",TNR))
accuracy <- sum(diag(cmSVM.factor))/sum(cmSVM.factor)
print(paste("整體準確率=",round(accuracy,2)))
}else if(length(cmSVM.factor)==2){
cm.factor
precision <- (cmSVM.factor[[2]]/sum(cmSVM.factor[,1]))
print(paste("預測",mycolName[2],"的正確率,precision=",round(precision,2)))
TPR <- (cmSVM.factor[[2]]/sum(cmSVM.factor[2,]))
print(paste("預測",mycolName[2],"的正確率,TPR=",round(TPR,2)))
accuracy <- sum(diag(cmSVM.factor)[[2]])/sum(cmSVM.factor[,1])
print(paste("整體準確率=",round(accuracy,2)))
}
cmSVM.factor
}
plot(formula=Petal.Length~Petal.Width,data=iris,col=iris$Species)
packageName="ggplot"
if(!(packageName %in% rownames(installed.packages()))){
install.packages(packageName)
}
data<-iris[-5]
km<-kmeans(data,centers=3,nstart = 10)
plot(formula =Petal.Length~Petal.Width,data=data,col=km$cluster,main="鳶尾花(iris)分群",xlab="Petal.Width(花瓣寬度)",
ylab="Petal.Length(花瓣長度)")
library("ggplot2")
ggplot(data,aes(x=Petal.Length,y=Petal.Width))+geom_point(aes(x=Petal.Length,y=Petal.Width))+
geom_point(aes(color=factor(km$cluster)))
WSS<-km$tot.withinss
WSS
BSS<- km$betweenss
BSS
TSS<- BSS+WSS
TSS
ratio<-WSS/TSS
ratio
=======
accuracies <- sapple(klist,knnFunction,knnTrain=knnTrain,knnTest=knnTest,trainLabels=trainLabels,testLabels=testdata$status)
df <- data.frame(kv=klist,accuracy=accuracies)
install.packages("ggplot2")
library(ggplot2)
ggplot(df,aes(x=kv,y=accuracy,label=kv,color=accuracy))+geom-point(size=5)+geom_text(vjust=2)
version
install.packages("ggplot2")
source('D:/r place/knnForW10.R')
source('D:/r place/knnForW10.R')
source('D:/r place/knnForW10.R')
source('D:/r place/knnForW10.R')
source('D:/r place/knnForW10.R')
install.packages("ggplot2")
source('D:/r place/knnForW10.R')
install.packages("tidyr")
source('D:/r place/knnForW10.R')
install.packages("ggplot2")
source('D:/r place/knnForW10.R')
source('D:/r place/knnForW10.R')
source('D:/r place/knnForW10.R')
source('D:/r place/knnForW10.R')
source('D:/r place/knnForW10.R')
source('D:/r place/knnForW10.R')
source('D:/r place/knnForW10.R')
install.packages("tidyr")
source('D:/r place/knnForW10.R')
source('~/.active-rstudio-document', encoding = 'UTF-8')
source('~/.active-rstudio-document', encoding = 'UTF-8')
source('~/.active-rstudio-document', encoding = 'UTF-8')
source('~/.active-rstudio-document', encoding = 'UTF-8')
source('~/.active-rstudio-document', encoding = 'UTF-8')
source('~/.active-rstudio-document', encoding = 'UTF-8')
source('~/.active-rstudio-document', encoding = 'UTF-8')
source('~/.active-rstudio-document', encoding = 'UTF-8')
source('~/.active-rstudio-document', encoding = 'UTF-8')
source('~/.active-rstudio-document', encoding = 'UTF-8')
source('D:/r place/BayesianTheoryForW10.R', encoding = 'UTF-8')
source('D:/r place/BayesianTheoryForW10.R', encoding = 'UTF-8')
source('D:/r place/BayesianTheoryForW10.R', encoding = 'UTF-8')
source('D:/r place/BayesianTheoryForW10.R', encoding = 'UTF-8')
source('D:/r place/BayesianTheoryForW10.R', encoding = 'UTF-8')
source('D:/r place/BayesianTheoryForW10.R', encoding = 'UTF-8')
source('D:/r place/BayesianTheoryForW10.R', encoding = 'UTF-8')
source('D:/r place/BayesianTheoryForW10.R', encoding = 'UTF-8')
source('D:/r place/BayesianTheoryForW10.R', encoding = 'UTF-8')
source('D:/r place/BayesianTheoryForW10.R', encoding = 'UTF-8')
source('D:/r place/BayesianTheoryForW10.R', encoding = 'UTF-8')
source('D:/r place/BayesianTheoryForW10.R', encoding = 'UTF-8')
source('D:/r place/BayesianTheoryForW10.R', encoding = 'UTF-8')
source('D:/r place/BayesianTheoryForW10.R', encoding = 'UTF-8')
source('D:/r place/BayesianTheoryForW10.R', encoding = 'UTF-8')
source('D:/r place/BayesianTheoryForW10.R', encoding = 'UTF-8')
source('D:/r place/BayesianTheoryForW10.R', encoding = 'UTF-8')
source('D:/r place/BayesianTheoryForW10.R', encoding = 'UTF-8')
source('D:/r place/BayesianTheoryForW10.R', encoding = 'UTF-8')
source('D:/r place/BayesianTheoryForW10.R', encoding = 'UTF-8')
source('D:/r place/BayesianTheoryForW10.R', encoding = 'UTF-8')
source('D:/r place/BayesianTheoryForW10.R', encoding = 'UTF-8')
source('D:/r place/BayesianTheoryForW10.R', encoding = 'UTF-8')
>>>>>>> d0963bb96b542fcf5479ec77e110b70426c3691d
